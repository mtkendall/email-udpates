{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing everything from email\n",
    "\n",
    "- non-Python pre-req was assigning every update email to a specific label, and then using Google Takeout to download that folder in an .mbox format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mailbox\n",
    "import base64\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime as dt\n",
    "from dateutil import tz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_email(string):\n",
    "    index = [m.start() for m in re.finditer('<', string)]\n",
    "    if len(index) > 0:\n",
    "        return string[:index[0]-1]\n",
    "    else: \n",
    "        return string\n",
    "\n",
    "def decode_string(string):\n",
    "    base64_bytes = string.encode(\"utf8\") \n",
    "    sample_string_bytes = base64.b64decode(base64_bytes) \n",
    "    sample_string = sample_string_bytes.decode(\"utf8\") \n",
    "    return sample_string\n",
    "\n",
    "def subject_decode(subject_string):\n",
    "    start_index = np.array([m.end() for m in re.finditer('=\\?UTF-8\\?B\\?', subject_string)])\n",
    "    if len(start_index) == 0:\n",
    "        return subject_string\n",
    "    else:\n",
    "        end_index = np.array([m.end() for m in re.finditer('\\?', subject_string)])\n",
    "        decoded_string = ''\n",
    "        for start in start_index:\n",
    "            end = np.array([x for x in end_index if x > start]).min()\n",
    "            decoded_string = decoded_string + decode_string(subject_string[start:end-1])\n",
    "        return decoded_string\n",
    "    \n",
    "def getbody(message): #getting plain text 'email body'\n",
    "    body = None\n",
    "    if message.is_multipart():\n",
    "        for part in message.walk():\n",
    "            if part.is_multipart():\n",
    "                for subpart in part.walk():\n",
    "                    if subpart.get_content_type() == 'text/plain':\n",
    "                        body = subpart.get_payload(decode=True)\n",
    "            elif part.get_content_type() == 'text/plain':\n",
    "                body = part.get_payload(decode=True)\n",
    "    elif message.get_content_type() == 'text/plain':\n",
    "        body = message.get_payload(decode=True)\n",
    "    return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "mbox = mailbox.mbox(\"squid_updates.mbox\")\n",
    "\n",
    "df_email = pd.DataFrame(columns = ['author', 'date', 'subject', 'content'])\n",
    "\n",
    "for i, message in enumerate(mbox):\n",
    "\n",
    "    dict = {}\n",
    "    dict['author'] = [message['from']]\n",
    "    dict['date'] = [message['date']]\n",
    "    dict['subject'] = [message['subject']]\n",
    "    dict['content'] = [getbody(message)]\n",
    "\n",
    "    df_email = pd.concat([df_email, pd.DataFrame.from_dict(dict)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_email_proc = (df_email\n",
    "    # Fixing date format\n",
    "    .assign(date = lambda x: pd.to_datetime(x['date'], format = '%a, %d %b %Y %H:%M:%S %z'))\n",
    "    .sort_values(by='date')\n",
    "    .assign(content = lambda x: x['content'].str.decode('latin1'))\n",
    "    .assign(subject = lambda y: y.apply(lambda x: subject_decode(x['subject']), axis = 1))\n",
    "    .assign(author = lambda y: y.apply(lambda x: remove_email(x['author']), axis = 1))\n",
    "    .assign(author = lambda x: x['author'].replace({'Matthew Kendall':'Matt Kendall','zachredmond18@gmail.com':'Zach Hyman', 'bryanfiori2015@u.northwestern.edu':'Bryan Fiori Ryan'}))\n",
    "    # Some basic filtering. Removing replies, short emails that shouldn't be in there, and the one time Sid forwarded my original update\n",
    "    .loc[lambda x: ~x['subject'].str.startswith('Re:')]\n",
    "    .loc[lambda x: x['subject']!='Fwd: Life Update Round 1']\n",
    ")\n",
    "\n",
    "zach_part_1_subject = df_email_proc.loc[(df_email_proc['author']=='Zach Hyman') & (df_email_proc['subject']=='Life Update 4, now with GIFs - Part 1') & (df_email_proc['date'] < dt.datetime(2016, 5, 10, tzinfo = tz.gettz('America/Chicago'))), 'subject'].values[0]\n",
    "zach_part_2_subject = df_email_proc.loc[(df_email_proc['author']=='Zach Hyman') & (df_email_proc['subject']=='Life Update Part 2') & (df_email_proc['date'] < dt.datetime(2016, 5, 10, tzinfo = tz.gettz('America/Chicago'))), 'subject'].values[0]\n",
    "zach_comb_subject = zach_part_1_subject + '\\n' + zach_part_2_subject\n",
    "\n",
    "zach_part_1_content = df_email_proc.loc[(df_email_proc['author']=='Zach Hyman') & (df_email_proc['subject']=='Life Update 4, now with GIFs - Part 1') & (df_email_proc['date'] < dt.datetime(2016, 5, 10, tzinfo = tz.gettz('America/Chicago'))), 'content'].values[0]\n",
    "zach_part_2_content = df_email_proc.loc[(df_email_proc['author']=='Zach Hyman') & (df_email_proc['subject']=='Life Update Part 2') & (df_email_proc['date'] < dt.datetime(2016, 5, 10, tzinfo = tz.gettz('America/Chicago'))), 'content'].values[0]\n",
    "zach_comb_content = zach_part_1_content + '\\n' + zach_part_2_content\n",
    "\n",
    "df_email_proc.loc[(df_email_proc['author']=='Zach Hyman') & (df_email_proc['subject']=='Life Update 4, now with GIFs - Part 1') & (df_email_proc['date'] < dt.datetime(2016, 5, 10, tzinfo = tz.gettz('America/Chicago'))), 'subject'] = zach_comb_subject\n",
    "df_email_proc.loc[(df_email_proc['author']=='Zach Hyman') & (df_email_proc['subject']=='Life Update 4, now with GIFs - Part 1') & (df_email_proc['date'] < dt.datetime(2016, 5, 10, tzinfo = tz.gettz('America/Chicago'))), 'content'] = zach_comb_content\n",
    "\n",
    "df_email_proc = df_email_proc.loc[~((df_email_proc['author']=='Zach Hyman') & (df_email_proc['subject']=='Life Update Part 2') & (df_email_proc['date'] < dt.datetime(2016, 5, 10, tzinfo = tz.gettz('America/Chicago')))), :]\n",
    "df_email_proc = df_email_proc.reset_index(drop=True)\n",
    "\n",
    "# df_email_proc.to_csv('squid_update_dwh.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>subject</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Matthew Kendall</td>\n",
       "      <td>2015-10-05 13:13:36-05:00</td>\n",
       "      <td>Life Update Round 1</td>\n",
       "      <td>Hey ya'll,\\n\\nHope your weeks are getting off ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jordan Scott</td>\n",
       "      <td>2015-10-12 09:35:42-05:00</td>\n",
       "      <td>Life Update Round 2</td>\n",
       "      <td>HEY FUCKERS\\n\\n\\nIâm going to mostly follow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Andrew Beir</td>\n",
       "      <td>2015-10-19 14:36:28-04:00</td>\n",
       "      <td>Life Update Round 3</td>\n",
       "      <td>Hello fellow squids,\\n\\nHad a hell of a weeken...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Daniel Ranti</td>\n",
       "      <td>2015-10-19 22:09:53-04:00</td>\n",
       "      <td>Life Update Part 3B</td>\n",
       "      <td>Fellow Squids,\\n\\nBuckle up ricky fucking tick...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eric Yang</td>\n",
       "      <td>2015-10-26 11:46:49-05:00</td>\n",
       "      <td>Life Update Round 4</td>\n",
       "      <td>Fellow Calamari,\\n\\nFirst off, I apologize for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>Jordan Scott</td>\n",
       "      <td>2022-08-11 19:33:30-06:00</td>\n",
       "      <td>lets go brandon (tennis)</td>\n",
       "      <td>LGB(T) Qommunity,\\n\\nfor once, a lot has happe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>Eric Yang</td>\n",
       "      <td>2022-08-18 22:40:24-07:00</td>\n",
       "      <td>Back to ɴᴏʀᴍᴀʟ Mode</td>\n",
       "      <td>Beep Boop. Doing something new this time aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>Daniel Ranti</td>\n",
       "      <td>2022-08-22 16:52:46-04:00</td>\n",
       "      <td>Are we still sending these via email too</td>\n",
       "      <td>https://docs.google.com/document/d/1jpnmv3u0TL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>\"Matt Felz (via Google Docs)\"</td>\n",
       "      <td>2023-10-27 04:08:42+00:00</td>\n",
       "      <td>Document shared with you: \"Felz 2023/10/26 Spo...</td>\n",
       "      <td>I've shared an item with you:\\n\\nFelz 2023/10/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>\"Siddharth Daftary (via Google Docs)\"</td>\n",
       "      <td>2023-11-02 05:37:11+00:00</td>\n",
       "      <td>Document shared with you: \"[Siddhu] Q4'23 Update\"</td>\n",
       "      <td>I've shared an item with you:\\n\\n[Siddhu] Q4'2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>298 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    author                       date  \\\n",
       "0                          Matthew Kendall  2015-10-05 13:13:36-05:00   \n",
       "1                             Jordan Scott  2015-10-12 09:35:42-05:00   \n",
       "2                              Andrew Beir  2015-10-19 14:36:28-04:00   \n",
       "3                             Daniel Ranti  2015-10-19 22:09:53-04:00   \n",
       "4                                Eric Yang  2015-10-26 11:46:49-05:00   \n",
       "..                                     ...                        ...   \n",
       "293                           Jordan Scott  2022-08-11 19:33:30-06:00   \n",
       "294                              Eric Yang  2022-08-18 22:40:24-07:00   \n",
       "295                           Daniel Ranti  2022-08-22 16:52:46-04:00   \n",
       "296          \"Matt Felz (via Google Docs)\"  2023-10-27 04:08:42+00:00   \n",
       "297  \"Siddharth Daftary (via Google Docs)\"  2023-11-02 05:37:11+00:00   \n",
       "\n",
       "                                               subject  \\\n",
       "0                                  Life Update Round 1   \n",
       "1                                  Life Update Round 2   \n",
       "2                                  Life Update Round 3   \n",
       "3                                  Life Update Part 3B   \n",
       "4                                  Life Update Round 4   \n",
       "..                                                 ...   \n",
       "293                           lets go brandon (tennis)   \n",
       "294                                Back to ɴᴏʀᴍᴀʟ Mode   \n",
       "295           Are we still sending these via email too   \n",
       "296  Document shared with you: \"Felz 2023/10/26 Spo...   \n",
       "297  Document shared with you: \"[Siddhu] Q4'23 Update\"   \n",
       "\n",
       "                                               content  \n",
       "0    Hey ya'll,\\n\\nHope your weeks are getting off ...  \n",
       "1    HEY FUCKERS\\n\\n\\nIâm going to mostly follow ...  \n",
       "2    Hello fellow squids,\\n\\nHad a hell of a weeken...  \n",
       "3    Fellow Squids,\\n\\nBuckle up ricky fucking tick...  \n",
       "4    Fellow Calamari,\\n\\nFirst off, I apologize for...  \n",
       "..                                                 ...  \n",
       "293  LGB(T) Qommunity,\\n\\nfor once, a lot has happe...  \n",
       "294  Beep Boop. Doing something new this time aroun...  \n",
       "295  https://docs.google.com/document/d/1jpnmv3u0TL...  \n",
       "296  I've shared an item with you:\\n\\nFelz 2023/10/...  \n",
       "297  I've shared an item with you:\\n\\n[Siddhu] Q4'2...  \n",
       "\n",
       "[298 rows x 4 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_email_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grabbing everything from Google docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If modifying these scopes, delete the file token.json.\n",
    "SCOPES = [\"https://www.googleapis.com/auth/documents.readonly\", \"https://www.googleapis.com/auth/drive\", \"https://www.googleapis.com/auth/drive.file\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'credentials.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m   creds\u001b[38;5;241m.\u001b[39mrefresh(Request())\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 12\u001b[0m   flow \u001b[38;5;241m=\u001b[39m \u001b[43mInstalledAppFlow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_client_secrets_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcredentials.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSCOPES\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m   creds \u001b[38;5;241m=\u001b[39m flow\u001b[38;5;241m.\u001b[39mrun_local_server(port\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Save the credentials for the next run\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mtkendall\\software\\miniconda3\\envs\\driver-score-monitoring\\lib\\site-packages\\google_auth_oauthlib\\flow.py:198\u001b[0m, in \u001b[0;36mFlow.from_client_secrets_file\u001b[1;34m(cls, client_secrets_file, scopes, **kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_client_secrets_file\u001b[39m(\u001b[38;5;28mcls\u001b[39m, client_secrets_file, scopes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;124;03m\"\"\"Creates a :class:`Flow` instance from a Google client secrets file.\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \n\u001b[0;32m    187\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m        Flow: The constructed Flow instance.\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 198\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclient_secrets_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[0;32m    199\u001b[0m         client_config \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_file)\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_client_config(client_config, scopes\u001b[38;5;241m=\u001b[39mscopes, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'credentials.json'"
     ]
    }
   ],
   "source": [
    "# Grab credentials.json from here: https://console.cloud.google.com/apis/credentials?project=dbt-learn-370222\n",
    "# Other documentation if needed: https://developers.google.com/docs/api/quickstart/python\n",
    "\n",
    "creds = None\n",
    "# The file token.json stores the user's access and refresh tokens, and is\n",
    "# created automatically when the authorization flow completes for the first\n",
    "# time.\n",
    "if os.path.exists(\"token.json\"):\n",
    "  creds = Credentials.from_authorized_user_file(\"token.json\", SCOPES)\n",
    "# If there are no (valid) credentials available, let the user log in.\n",
    "if not creds or not creds.valid:\n",
    "  if creds and creds.expired and creds.refresh_token:\n",
    "    creds.refresh(Request())\n",
    "  else:\n",
    "    flow = InstalledAppFlow.from_client_secrets_file(\n",
    "        \"credentials.json\", SCOPES\n",
    "    )\n",
    "    creds = flow.run_local_server(port=0)\n",
    "  # Save the credentials for the next run\n",
    "  with open(\"token.json\", \"w\") as token:\n",
    "    token.write(creds.to_json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plain_text(DOCUMENT_ID, creds):\n",
    "\n",
    "  try:\n",
    "    service = build(\"docs\", \"v1\", credentials=creds)\n",
    "\n",
    "    # Retrieve the documents contents from the Docs service.\n",
    "    document = service.documents().get(documentId=DOCUMENT_ID).execute()\n",
    "\n",
    "    file = document.get('body')\n",
    "    \n",
    "    content_string = ''\n",
    "    for element in file['content']:\n",
    "        if 'paragraph' in element:\n",
    "            paragraph = element['paragraph']\n",
    "            for element in paragraph['elements']:\n",
    "                if 'textRun' in element:\n",
    "                    content_string = content_string + element['textRun']['content']\n",
    "\n",
    "    return content_string\n",
    "\n",
    "  except HttpError as err:\n",
    "    print(err)\n",
    "\n",
    "def get_file_info_from_id(fileId, service):\n",
    "    try:\n",
    "\n",
    "        response = service.files().get(fileId=fileId, fields=\"id, name, owners, createdTime\").execute()\n",
    "\n",
    "        df = pd.DataFrame(columns = ['author', 'date', 'subject', 'fileId'])\n",
    "\n",
    "        item = response\n",
    "\n",
    "        dict = {}\n",
    "        dict['fileId'] = fileId\n",
    "        dict['subject'] = item['name']\n",
    "        dict['author'] = item['owners'][0]['displayName']\n",
    "\n",
    "        created_at = pd.to_datetime(item['createdTime'])\n",
    "\n",
    "        comment_dates = np.array([])\n",
    "        comments = service.comments().list(fileId=fileId,fields='comments').execute()      \n",
    "        for comment in comments.get('comments'):         \n",
    "            comment_dates = np.append(comment_dates, pd.to_datetime(comment['createdTime']))\n",
    "\n",
    "        if len(comment_dates) > 0:\n",
    "            created_at = comment_dates.min()\n",
    "\n",
    "        dict['date'] = created_at\n",
    "\n",
    "        dict = {k:[v] for k,v in dict.items()}\n",
    "        df = pd.concat([df, pd.DataFrame.from_dict(dict)], axis = 0)\n",
    "\n",
    "        return df\n",
    "    \n",
    "    except HttpError as error:\n",
    "        print(f\"An error occurred: {error}\")\n",
    "\n",
    "def get_file_info_from_folder(folderId, creds):\n",
    "\n",
    "  try:\n",
    "    service = build(\"drive\", \"v3\", credentials=creds)\n",
    "  \n",
    "    response = service.files().list(q=f\"'{folderId}' in parents\", pageSize=1000, fields=\"files(id, mimeType, name, owners, createdTime)\").execute()\n",
    "    items = response.get('files', [])\n",
    "\n",
    "    df = pd.DataFrame(columns = ['author', 'date', 'subject', 'fileId'])\n",
    "\n",
    "    for item in items:\n",
    "      if item['mimeType'] == 'application/vnd.google-apps.document':\n",
    "        dict = {}\n",
    "        dict['fileId'] = item['id']\n",
    "        dict['subject'] = item['name']\n",
    "        dict['author'] = item['owners'][0]['displayName']\n",
    "\n",
    "        created_at = pd.to_datetime(item['createdTime'])\n",
    "\n",
    "        comment_dates = np.array([])\n",
    "        comments = service.comments().list(fileId=item['id'],fields='comments').execute()      \n",
    "        for comment in comments.get('comments'):         \n",
    "          comment_dates = np.append(comment_dates, pd.to_datetime(comment['createdTime']))\n",
    "\n",
    "        if len(comment_dates) > 0:\n",
    "          created_at = comment_dates.min()\n",
    "\n",
    "        dict['date'] = created_at\n",
    "\n",
    "        dict = {k:[v] for k,v in dict.items()}\n",
    "        df = pd.concat([df, pd.DataFrame.from_dict(dict)], axis = 0)\n",
    "\n",
    "    return df\n",
    "      \n",
    "  except HttpError as error:\n",
    "    print(f\"An error occurred: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = [\n",
    "    '1hhpQ2wQ8jGLrfJnr237dNnhwvTLKib6Htfb2_qCQrns',\n",
    "    '1jtEpNKolr3_bxhSWygp5BKcXAJr7ELY3ZQWIKy7aWec',\n",
    "    '1ZvNQFsFPfPZh2rUq1fEI4D5FmZ-RJxXC5tbNvGNKQGo'\n",
    "]\n",
    "\n",
    "service = build(\"drive\", \"v3\", credentials=creds)\n",
    "\n",
    "df_drive_list = pd.DataFrame(columns = ['author', 'date', 'subject', 'fileId'])\n",
    "\n",
    "for doc in id_list:\n",
    "    df_drive_list = pd.concat([df_drive_list, get_file_info_from_id(doc, service)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drive_folder = get_file_info_from_folder('1s2eD0iWge9Uur6GXRQjtHWgG47JryYkR', creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drive = pd.concat([df_drive_list, df_drive_folder]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drive_proc = (\n",
    "    df_drive\n",
    "    .sort_values(by='date')\n",
    "    .assign(content = lambda y: y.apply(lambda x: get_plain_text(x['fileId'], creds), axis = 1))\n",
    "    .drop('fileId', axis=1)\n",
    ")\n",
    "\n",
    "df_drive_proc = df_drive_proc.loc[~((df_drive_proc['author']=='Jordan Scott') & (df_drive_proc['subject']==\"[JORDAN] let's go brandon (tennis)\") & (df_drive_proc['date'] < dt.datetime(2022, 8, 20, tzinfo = tz.gettz('America/Chicago')))), :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb = (\n",
    "    pd.concat([df_email_proc, df_drive_proc])\n",
    "    .assign(word_count = lambda x: x['content'].str.split().str.len())\n",
    "    .loc[lambda x: x['word_count']>100]\n",
    "    .sort_values(by='date')\n",
    "    .assign(days_since_prior = lambda x: (x['date'] - x['date'].shift(1))/np.timedelta64(1, 'D'))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comb.to_csv('squid_updates.csv', encoding = 'utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "driver-score-monitoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
